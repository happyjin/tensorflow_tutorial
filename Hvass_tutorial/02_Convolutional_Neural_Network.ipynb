{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional layer 1\n",
    "filter_size1 = 5\n",
    "num_filters1 = 16\n",
    "\n",
    "# convolutional layter 2\n",
    "filter_size2 = 5\n",
    "num_filters2 = 36\n",
    "\n",
    "# full-connected layer\n",
    "fc_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\nExtracting data/MNIST/t10k-images-idx3-ubyte.gz\nExtracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of:\n- training-set:\t55000\n- Test-set:\t\t10000\n- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "print(\"size of:\")\n",
    "print(\"- training-set:\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(data.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.validation.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test.cls = np.argmax(data.test.labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "num_channels = 1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # create figure with 3x3 sub-plots\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    plt.clf()\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # plot the image\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "        # show the true and predicted classes\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "        ax.set_xlabel(xlabel)\n",
    "        # remove ticks from the plot\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a few images to see if data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAD5CAYAAAC9FVegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ee18cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f2d1a50>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first set images from the test-set\n",
    "images = data.test.images[0:9]\n",
    "# get tht true classes for those images\n",
    "cls_true = data.test.cls[0:9]\n",
    "# plot the images and labels using the lot_images function\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper-functions for creating new variables\n",
    "create graph Variables in Tensorflow and initialize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weight variables and initialize it\n",
    "def new_weights(shape):\n",
    "    return tf.Variabfle(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "# define biases variables and initialize it\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper-function for creating a new Convolutional Layer\n",
    "The function creates a new convolutional layer in the computational graph for Tensorflow. Nothing is actually calculated here, we are just adding the mathmatical formulas to the Tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,   # the previous layer\n",
    "                   num_input_channels, # Num. channels in prev. layer\n",
    "                   filter_size, # width and height of each filter\n",
    "                   num_filters, # number of filters\n",
    "                   use_pooling=True): # use 2x2 max-pooling\n",
    "    # shape of filter-weithts for the convolution\n",
    "    # this format is determined by the Tensorflow API, that is \n",
    "    # [filter_height, filter_width, in_channels, out_channels]\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    \n",
    "    # create new weights and initialize it, also known as filters with the given shape\n",
    "    weights = new_weights(shape=shape)\n",
    "    \n",
    "    # create new biases, one for each filter\n",
    "    biases = new_biases(length=num_filters)\n",
    "    \n",
    "    # create the TensorFlow operation for convolution\n",
    "    # strides: A list of `ints`. 1-D tensor of length 4.\n",
    "    # note that the strides are set to 1 in all dimensions\n",
    "    # the first and last stride must always be 1\n",
    "    # because the first is for the image-number and the last\n",
    "    # is for input-channel. But e.g. strides=[1,2,2,1] would \n",
    "    # mean [img_num, x-axis_stride, y-axis_stride, input_channel]\n",
    "    # padding='SAME': means the input image is padded with zeroes \n",
    "    # so the size of the output is the same\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "    \n",
    "    # add the biases to the results of convolution. A bias-value is added to each filter-channel\n",
    "    layer += biases\n",
    "    \n",
    "    # Use pooling to down-sampling the image resolution\n",
    "    # this is 2x2 max-pooling, which means that we consider 2x2 windows and select the\n",
    "    # largest value in each window. Then we move 2 pixels for the next window\n",
    "    if use_pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "    \n",
    "    # Rectified Linear Unit(ReLU)\n",
    "    # It calculates max(x,0) for each input pixel x.\n",
    "    # since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    # return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights lagter\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper-function for flattening a layer\n",
    "A convolutional layer produces an output tensor with 4 dimensions. we will add fully-connected layers after the convolutional layers, so we need to reduce the 4-D tensor to 2-D which can be used as input to the fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # get the shape of input layer\n",
    "    layer_shape = layer.get_shape()\n",
    "    \n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "    \n",
    "    # the number of features is: img_height * img_width * img_channels\n",
    "    # we can use a function from TensorFlow to calculate this\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # reshape the layer to [num_images, num_features]\n",
    "    # -1 means the size in that dimension is calculated\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    \n",
    "    # return both the flattened layer and the number of features\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper-function for creating a new Fully-connected layer\n",
    "<p>This function creates a new fully-connected layer in the computational graph for TensorFlow. Nothing is actually calculated here. We are just adding the mathmatical formulars to the TensorFlow graph.\n",
    "</p>\n",
    "It is assumed that the input is a 2-D tensor of shape [num_images, num_inputs]. The output is a 2-D tensor of shape [num_images, num_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input, # the previous layer\n",
    "                 num_inputs, # num of inputs from prev. layer\n",
    "                 num_outputs, # num of outputs\n",
    "                 use_relu=True): # use Rectified Linear Unit (ReLU)?\n",
    "    \n",
    "    # create new weights and biases and initialize them \n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "    \n",
    "    # calculate the layer as the matrix multiplication of the input and weights, \n",
    "    # and then add the bias-values\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    \n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## placeholder variables\n",
    "placeholder variables serve as the input to the TensorFlow computational graph that we may change each time we excute the graph. We call this the placeholder variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [None, img_size_flat]: None means the tensor may hold arbitrary number of images with\n",
    "# each image being a vector of length img_size_flat\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "\n",
    "# Since the convolutional layers expect x to be encoded as a 4-D tensor so we have to reshape it instead\n",
    "# [num_images, img_height, img_width, num_channels].\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "# placeholder variable for the true labels associated with the images that were\n",
    "# input in the placeholder variable x\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "# placeholder variable for true class-number\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convolutional layer 1\n",
    "create the first convolutional layer. It takes x_image as input and creates num_filters1 different filters. each have width and height eqaul to filter_size1. Finally, we utilize down-sampling and 2x2 max-pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = new_conv_layer(input=x_image,\n",
    "                                            num_input_channels=num_channels,\n",
    "                                            filter_size=filter_size1,\n",
    "                                            num_filters=num_filters1,\n",
    "                                            use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_4:0' shape=(?, 14, 14, 16) dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of tensor that is (?,14,14,16) which means that\n",
    "# the output is an arbitrary number of images (signed by ?)\n",
    "layer_conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer2\n",
    "create the second convolutional layer, which takes as input the output from the first convolutional layer. The numebr of input channels corresponds to the number of filters in the first convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv2, weights_conv2 = new_conv_layer(input=layer_conv1,\n",
    "                                            num_input_channels=num_filters1,\n",
    "                                            filter_size=filter_size2,\n",
    "                                            num_filters=num_filters2,\n",
    "                                            use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_5:0' shape=(?, 7, 7, 36) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the output shape of the convolutional layer2\n",
    "layer_conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Layer which is prepare for the fully-connected layer\n",
    "The convolutional layers output 4-D tensors. We now wish to use these as input in a fully-connected network, which requires for the tensors to be reshaped or flattened to 2-D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check that the tensor now have shape (?, 1764) which means there's an arbitrary number of images which have been flattened to vectors of length 1764 each. Note that 1764 = 7x7x36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_4:0' shape=(?, 1764) dtype=float32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1764"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-connected layer 1\n",
    "add a fully-connected layer to the network. The input is the flattened layer from the previous convolution. The numeber of neurons or nodes in the fully-connected connected layer is fc_size. ReLU is used so that we can learn non-linear relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features, # for one image, there will be num_features as input\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_8:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the fully-connected layer shape\n",
    "layer_fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fully-connected layer 2\n",
    "add fully-connected layer that outputs vectors of length 10 for determining which of the 10 classes the input image belongs to. Note that ReLU is not used in this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_10:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicted classes\n",
    "The second fully-connected layer estimates how likely that the input image belongs to each of the 10 classes. However, this is very diffcult to interprete because the numbers may be very small or large, so we want to normalize them so that each elements is limited between 0 and 1. And the 10 elements sum to 1. This is calculated using the so-called softmax function and the result is stored in y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the class-number is the index of the largest element\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost-function to be optimized\n",
    "<p>The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if predicted output of the model exactly matches the desired output then the cross-entropy equals to 0. The goal of optimization is therefore to minimize the cross-entropy so it gets as close to 0 as possible by changing the variables of the network layers.</p>\n",
    "Note that the build-in function to compute cross-entropy in the TensorFlow calculates the softmax internally so we must use the output of layer_fc2 directly rather than y_pred which has already has the softmax applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimization method\n",
    "<p>Now we have a cost measure that must be minimized, we can create an optimizer. In this case it is the AdamOptimizer which is an advanced form of Gradient Descent</p>\n",
    "Note that optimization is not performed at this point, nothing is calculated at all, we just add the optimizer-obeject to the TensorFlow graph for later excution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create TensorFlow session\n",
    "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize variables\n",
    "The variables for weights and biases must be initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
